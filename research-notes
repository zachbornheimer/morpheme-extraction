52 minutes, 376K words, 36K were unique.  
Some of these unique words are not unique.
  Case Matters
  Some words might be stuck together (like: thesewords)
  Some puncutation might be fusing words (like: this,due)

Plan of attack for identifing actually unique words:

Parse in a dictionary.
Words that occur once are most likely invalid.

====================

Because of some recent additions in tracking words around a target word, 
I have been forced to use the USF Research Computing Cluster to get this done.
It took ~2 hours for it to parse __Moby Dick__, __The Advetures of Huckleberry Finn__, and __War of the Worlds__.
Because of this, I have implemented a very dangerous, but usable way of storing and loading the data.
Because it takes unrealistically long to parse the data, the program parses it once then stores the actual variable in
restore.zy (or an alternate filename) and then it is loaded into $_ via a for loop and evaluated.
This is increadibly dangerous, but it is the only solution until Zedram is done.

====================

To read the data and store it in the file, it took ~241 minutes for the 376K words and the word frequencies.


====================
Nov. 11

Latest results:
~403K words, ~28K unique.  Forced to rewrite the storage mechanism becuase it would cause memory overflows with large datasets.  Program passed tests.

Also, figured out the latest flows for the program and some of the impressive implications. See README.md for details.

====================
Nov. 25

Interestingly enough, this algorithm seems to function as a completly universal algorithm for learning language.  No matter what the language, it seems that this algorithm should be able to parse it (given that it uses an alphabet).  Without strain, this could easily be extended to speech.

I'm currently working on the extrapolation procedure.
I am forced to explicitly define the four classes of morphemes: prefix, suffix, stem, and infix.  I'm not thrilled with that especially since it requires that I hardcode the algorithm into the code.

The algorithm looks like this:
If the regex ends with the morpheme and then an end-of-line char OR morpheme then a set of characters then a *, it is a suffix.
If the inverse is true, it is a prefix.

Theoretically, no word will be in the same context and change its presentation.  If the word appears in one context it should stay the same (meaning that you aren't going to have different prefixes and suffixes for the same stem in the same context).  If you do, enough data will rectify that misinformation.

Stems can be isolated by removing morphemes and substituting them by checking to see if they are valid words.  That, however is not going to be a part of this first round of experimentation.

====================
Nov. 29

I thought I should clarify why I chose to use a grammar and extend the grammar with prefixes, suffixes, infixes, and roots (and also why not circumfixes).  Basically, all languages need to terminate their words and start their words.  All words, no matter the language can be matched (in the most general yet specific way possible) as followed:
/(<prefix>*)<root>+(<suffix>*)/

I can explain this tautologically, but it is unnecessary.  For the most part, everything can be explained in these categories.  Circumfixes and infixes, however, cannot.  Circumfixes will be handled by the probabilites.  For example, if it is a circumfix, what is the probability the suffix and prefix will appear together?  What about apart?  What about in this particular context?  The problem will be solved with ease using this method.
The only challenge is infixes.  The concept I will use is identifing if it can be represented with .*<infix>.* then it is an infix.  Both .*s will be captured and a character class will be made.  This is the easiest method (even if it turns out to be more verbose).  The problem with infixation is that it may rely on more than just the previous character.  Character classes will be created for each character preceeding the infix:
/[aeiou][mn]et[lr][d]/

Once again, once the alphabet is discovered, character classes can begin to be generalized: <all-letters>, <nm>, etc.   This allows for optimization of storage.

Finally, it is imperative that this be done because it may descover the true character classes of the language (and it, of course, will misname them because it doesn't understand what vowels are).
======
Unrelated.
I found that I am unable to do dynamic grammar naming in perl6 (at least as of now).  I'm going to try and contact Larry Wall to see what his reasoning behind it is or if it is just an implementation issue.  THe workaround (and possibly best solution is to have the program write the lines for the grammar and store it in a file.  It will then slurp and eval.  I may have to do some modifcations to make sure it doesn't breech available memory.  We'll cross that bridge in due time.

====================y
Dec. 1

I have it extracting infixes, prefixes, and suffixes.
Circumfixes will be handled through frequencies, specifically given a prefix and suffix that occur in the same frequency for a particualr situation.  This will be very rare if the data set is large enough...which will give a good guess towards circumfixes.  Further, global morphemes will help.  If certain prefixes and suffixes are ONLY seen in the presence of one another, they can be, with relative certainty, be called circumfixes.  From a global scope, this can also be narrowed to just a particular context...and it will work just the same.  In fact, a hash table with guesses towards a words's context (being circumfix or stem) can be handled as well.
Remember that the stem will never be identifiable directly because the algorithm looks for patterns in two words, but those words won't be the same.  The only way it will identify a stem is if the stem occurs like: /^(thought)s/ and /(fore)thought$/ and even in this situation, it could get tricky.  It will automatically consider a word a stem if it occurs in both the prefix and suffix positions to another morpheme.  This is the only way to access the data...indirectly.

====================
Dec. 19, 2012

Created a style guide <http://zysys.org/wiki/index.php/Perl6_Style_Guide> for Perl6 and published it online.  Implemented those stylistic changes except for POD Documentation.  Because the REPL has a `help` function, I think that, for now, it is unnecessary.  After the optimization of the algorithms syntactically, there is a slight change to the order of data stored (so it would give a different order of morphemes, but the same content would be there).   These mods have been tested and, so far, no bugs have been found.  The syntactic changes made reflect the ideas in the style guide.  If this project expands to a project as large as I think it will need to be, I think that it is important to get some of this basic style out of the way early.
